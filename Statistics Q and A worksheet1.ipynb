{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. What do you understand by the term Normal Distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normal distribution is the most widely known and used of all distribution. Because the normal distribution approximates many natural phenomena so well, it has developed into a standard of reference for many probability problems.Many things actually distributed or very closed to it For example, height and intelligence are approxmately normally distributed, measurement errors also often have a normal distribution. The normal distribution is easy to work with mathematically.A normal distribution is a set of a continuous variable spread across a normal curve or in the shape of a bell curve. You can consider it as a continuous probability distribution which is useful in statistics. It is useful to analyze the variables and their relationships when we are using the normal distribution curve. The normal distribution is a probability function that describes how the values of a variable are distributed. It is a symmetric distribution where most of the observations cluster around the central peak and the probabilities for values further away from the mean taper off equally in both directions. Extreme values in both tails of the distribution are similarly unlikely. It is also known as the Gaussian distribution and the bell curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.How do you handle missing data? What imputation techniques do you recommend?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have three options when dealing with missing data. The most obvious and by far the easiest option, is to simply ignore any observations that have missing values. This is often called complete case analysis or listwise deletion of missing values.\n",
    "\n",
    "Another approach is to impute the missing values. This involves using statistical or machine learning models to make educated guesses based about the values of the missing data. For example, you could create a model that predicts the number of employees based on the other variables and then use this model to predict the number of employees. A variant of this approach, known as multiple imputation, is usually considered best practice when building regression-type models (e.g., linear regression, logistic regression).\n",
    "\n",
    "A third approach is to use analysis methods that are specifically designed to deal with missing values, such as latent class analysis. \n",
    "\n",
    "\n",
    "Imputation is the process of replacing the missing data with approximate values. Instead of deleting any columns or rows that has any missing value, this approach preserves all cases by replacing the missing data with the value estimated by other available information so we recommend\n",
    "1 Mean,Median Imputation\n",
    "2.Mode substitution\n",
    "4. Knn Imputer\n",
    "5. Iterative Imputer\n",
    "6. Simple Imputer With Indicator\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. What is A/B testing?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An AB test is an example of statistical hypothesis testing, a process whereby a hypothesis is made about the relationship between two data sets  A/B, and those data sets are then compared against each other to determine if there is a statistically significant relationship or not. A/B testing is a basic randomized control experiment. It is a way to compare the two versions of a variable to find out which performs better in a controlled environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Is mean imputation of missing data acceptable practice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes is mean imputation of missing data acceptable practice, imputing the mean preserves the mean of the observed data. So if the data are missing completely at random, the estimate of the mean remains unbiased by imputing mean is the replacement of a missing observation with the mean of the non-missing observations for that variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. What is linear regression in statistics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression is set of statistical processes for estimating the relationships among variables. Linear regression attempts to model the relationship between two variables by fitting a linear equation to observed data. One variable is considered to be an explanatory variable, and the other is considered to be a dependent variable. For example, a modeler might want to relate the weights of individuals to their heights using a linear regression model.. Linear reagression uses one independent variables to explain or predict the outcomes of the dependent variable Y. A valuable numerical measure of association between two variables is the correlation coefficient, which is a value between -1 and 1 indicating the strength of the association of the observed data for the two variables.\n",
    "   A linear regression line has an equation of the form Y = a + bX, where X is the explanatory variable and Y is the dependent variable. The slope of the line is b, and a is the intercept (the value of y when x = 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. What are the various branches of statistics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There two main branches of statistics are descriptive statistics and inferential statistics.\n",
    "\n",
    "Descriptive statistics is considered as the first part of statistical analysis which deals with collection and presentation of data. Scientifically, descriptive statistics can be defined as brief explanatory coefficients that are used by statisticians to summarize a given data set. Generally, a data set can either represent a sample of a population or the entire populations. Descriptive statistics can be categorized into\n",
    "\n",
    "Measures of central tendency\n",
    "Measures of variability\n",
    "\n",
    "Inferential Statistics\n",
    "Inferential statistics are techniques that enable statisticians to use the gathered information from a sample to make inferences, decisions or predictions about a given population. Inferential statistics often talks in probability terms by using descriptive statistics. These techniques are majorly used by statisticians to analyze data, make estimates and draw conclusions from the limited information which is obtained by sampling and testing how reliable the estimates are.\n",
    "The different types of calculation of inferential statistics include:\n",
    "\n",
    "Regression analysis\n",
    "Analysis of variance (ANOVA)\n",
    "Statistical significance (t-test)\n",
    "Correlation analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
